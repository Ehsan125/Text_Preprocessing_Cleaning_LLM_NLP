{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af641397-62f8-446b-9fba-f287a40ffae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The defendant was charged with robbery.', 'The crime took place in 2023.', 'The defense claims the defendant was not present.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\ehsan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# sys.path.append('C:/Users/muhammad.ehsan/hr_automation/Lib/site-packages')\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sentence_level_chunking(text):\n",
    "    sentences = sent_tokenize(text)  # Tokenize the text into sentences\n",
    "    return sentences\n",
    "\n",
    "text = \"The defendant was charged with robbery. The crime took place in 2023. The defense claims the defendant was not present.\"\n",
    "chunks = sentence_level_chunking(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "125cfb75-a783-4e4c-af4a-924d255be659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The defendant was charged with robbery and assault.\\nThe crime took place on July 15, 2023, at the victim’s residence.', 'The prosecution argues that the defendant was seen fleeing the scene.\\nThe defense claims that the defendant was not present.']\n"
     ]
    }
   ],
   "source": [
    "def paragraph_level_chunking(text):\n",
    "    paragraphs = text.split(\"\\n\\n\")  # Split text by empty lines representing paragraphs\n",
    "    return paragraphs\n",
    "\n",
    "text = \"\"\"The defendant was charged with robbery and assault.\n",
    "The crime took place on July 15, 2023, at the victim’s residence.\n",
    "\n",
    "The prosecution argues that the defendant was seen fleeing the scene.\n",
    "The defense claims that the defendant was not present.\"\"\"\n",
    "chunks = paragraph_level_chunking(text)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4cb4f71-9569-4699-850a-ead7536df121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The defendant was charged with robbery'], []]\n"
     ]
    }
   ],
   "source": [
    "def topic_based_chunking(text, topics):\n",
    "    topic_chunks = []\n",
    "    for topic in topics:\n",
    "        topic_chunks.append([sentence for sentence in text.split('.') if topic in sentence])\n",
    "    return topic_chunks\n",
    "\n",
    "text = \"The defendant was charged with robbery. The crime took place in 2023. The defense claims the defendant was not present. Witnesses saw the defendant fleeing.\"\n",
    "topics = [\"robbery\", \"witnesses\"]\n",
    "chunks = topic_based_chunking(text, topics)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "332b2f72-46cb-4a7a-a525-3282abf045d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The defendant was charged with robbery and assault. The crime', 'took place in 2023 at the victim’s residence. Witnesses saw', 'the defendant fleeing.']\n"
     ]
    }
   ],
   "source": [
    "def fixed_size_chunking(text, chunk_size=50):\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "text = \"The defendant was charged with robbery and assault. The crime took place in 2023 at the victim’s residence. Witnesses saw the defendant fleeing.\"\n",
    "chunks = fixed_size_chunking(text, chunk_size=10)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b72848fc-47fc-48f9-98d6-d67fedf6e67d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The defendant was charged with robbery. The crime took place in 2023.', 'The defense claims the defendant was not present.Witnesses saw the defendant fleeing.']\n"
     ]
    }
   ],
   "source": [
    "def context_aware_chunking(text, threshold=50):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk + sentence) < threshold:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "\n",
    "    return chunks\n",
    "\n",
    "text = \"The defendant was charged with robbery. The crime took place in 2023. The defense claims the defendant was not present. Witnesses saw the defendant fleeing.\"\n",
    "chunks = context_aware_chunking(text, threshold=100)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5827810-1758-43f1-8d5c-6f3d768155e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The defendant was charged with robbery. The crime took place', 'in 2023. Witnesses saw the defendant fleeing. The defense claims', 'the defendant was not present.']\n"
     ]
    }
   ],
   "source": [
    "def hybrid_chunking(text, chunk_size=50):\n",
    "    sentences = sent_tokenize(text)\n",
    "    all_sentences = ' '.join(sentences)  # Sentence-level chunking first\n",
    "    words = all_sentences.split()  # Fixed-size chunking on top\n",
    "    return [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "text = \"The defendant was charged with robbery. The crime took place in 2023. Witnesses saw the defendant fleeing. The defense claims the defendant was not present.\"\n",
    "chunks = hybrid_chunking(text, chunk_size=10)\n",
    "print(chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60f04965-f0bc-44e9-8186-06523082b5a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The defendant was charged with robbery. The crime took place',\n",
       " 'in 2023. Witnesses saw the defendant fleeing. The defense claims',\n",
       " 'the defendant was not present.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b74df77a-fbe8-432b-9181-f769ff2c8bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ehsan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The defendant was charged with robbery.\n",
      "The crime took place in 2023.\n",
      "The defense claims the defendant was not present at the crime scene.\n",
      "The prosecution has witnesses that saw the defendant leaving the location of the crime.\n",
      "The court proceedings started in early 2023.\n",
      "Sentences can vary based on various factors, such as the severity of the crime and criminal history.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load the pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Context-aware chunking using embeddings\n",
    "def context_aware_embedding_chunking(text, similarity_threshold=0.7, max_chunk_size=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_embedding = np.zeros_like(embeddings[0])\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        current_embedding = embeddings[i]\n",
    "        \n",
    "        if len(current_chunk) == 0:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_embedding = current_embedding\n",
    "            continue\n",
    "        \n",
    "        similarity = cosine_similarity(current_chunk_embedding, current_embedding)\n",
    "        \n",
    "        if similarity > similarity_threshold and len(current_chunk) < max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_embedding = np.mean(model.encode(current_chunk), axis=0)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_embedding = current_embedding\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"The defendant was charged with robbery. The crime took place in 2023.\n",
    "The defense claims the defendant was not present at the crime scene. The prosecution has witnesses that saw the defendant leaving the location of the crime. \n",
    "The court proceedings started in early 2023. Sentences can vary based on various factors, such as the severity of the crime and criminal history.\"\"\"\n",
    "chunks = context_aware_embedding_chunking(text)\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "780eda85-2087-463e-93ec-2c0687ef4249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Downloading sentence_transformers-3.2.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
      "     ---------------------------------------- 0.0/44.4 kB ? eta -:--:--\n",
      "     ----------------------------------- -- 41.0/44.4 kB 991.0 kB/s eta 0:00:01\n",
      "     -------------------------------------- 44.4/44.4 kB 437.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from sentence-transformers) (4.65.0)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from sentence-transformers) (2.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from sentence-transformers) (1.11.1)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from sentence-transformers) (9.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.12.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2023.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
      "Requirement already satisfied: requests in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.25.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.6.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.4.5-cp311-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\muhammad.ehsan\\hr_automation\\lib\\site-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Downloading sentence_transformers-3.2.0-py3-none-any.whl (255 kB)\n",
      "   ---------------------------------------- 0.0/255.2 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 71.7/255.2 kB 2.0 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 153.6/255.2 kB 1.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 235.5/255.2 kB 1.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 255.2/255.2 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.25.2-py3-none-any.whl (436 kB)\n",
      "   ---------------------------------------- 0.0/436.6 kB ? eta -:--:--\n",
      "   ----------- ---------------------------- 122.9/436.6 kB 2.4 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 256.0/436.6 kB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 399.4/436.6 kB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 436.6/436.6 kB 2.7 MB/s eta 0:00:00\n",
      "Downloading transformers-4.45.2-py3-none-any.whl (9.9 MB)\n",
      "   ---------------------------------------- 0.0/9.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.1/9.9 MB 4.3 MB/s eta 0:00:03\n",
      "   - -------------------------------------- 0.3/9.9 MB 3.2 MB/s eta 0:00:04\n",
      "   - -------------------------------------- 0.4/9.9 MB 3.7 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/9.9 MB 3.3 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/9.9 MB 3.2 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/9.9 MB 2.8 MB/s eta 0:00:04\n",
      "   --- ------------------------------------ 0.9/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.9/9.9 MB 2.7 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.2/9.9 MB 2.9 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.3/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.5/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.8/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 2.0/9.9 MB 3.1 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.1/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.2/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.3/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.4/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.6/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.7/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.9/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.0/9.9 MB 3.0 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 3.1/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.2/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.5/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.6/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.7/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 3.9/9.9 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.1/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.3/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.4/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.5/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.7/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.8/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.0/9.9 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.2/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.4/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.5/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.7/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.8/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.9/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.2/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.3/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.4/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 6.5/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.7/9.9 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.9/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.0/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.2/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 7.4/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 7.5/9.9 MB 3.2 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.7/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.8/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/9.9 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/9.9 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/9.9 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.0/9.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 8.6/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.0/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 9.1/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 9.3/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.4/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.5/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 9.6/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.7/9.9 MB 3.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.9/9.9 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.9/9.9 MB 3.0 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.5-cp311-none-win_amd64.whl (285 kB)\n",
      "   ---------------------------------------- 0.0/286.0 kB ? eta -:--:--\n",
      "   --------------- ------------------------ 112.6/286.0 kB 6.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 276.5/286.0 kB 4.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 286.0/286.0 kB 2.9 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.20.1-cp311-none-win_amd64.whl (2.4 MB)\n",
      "   ---------------------------------------- 0.0/2.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.2/2.4 MB 6.9 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.4/2.4 MB 4.3 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 0.5/2.4 MB 4.2 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 0.7/2.4 MB 3.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 1.0/2.4 MB 3.9 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.2/2.4 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.4/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.5/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 1.6/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.8/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.9/2.4 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 2.1/2.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.4/2.4 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.4/2.4 MB 3.3 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "  Attempting uninstall: safetensors\n",
      "    Found existing installation: safetensors 0.3.1\n",
      "    Uninstalling safetensors-0.3.1:\n",
      "      Successfully uninstalled safetensors-0.3.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.16.4\n",
      "    Uninstalling huggingface-hub-0.16.4:\n",
      "      Successfully uninstalled huggingface-hub-0.16.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.13.3\n",
      "    Uninstalling tokenizers-0.13.3:\n",
      "      Successfully uninstalled tokenizers-0.13.3\n",
      "Successfully installed huggingface-hub-0.25.2 safetensors-0.4.5 sentence-transformers-3.2.0 tokenizers-0.20.1 transformers-4.45.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b7ee85a-975b-4ffd-89e0-2e0d2eb9a58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/muhammad.ehsan/hr_automation/Lib/site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62b2329f4b0a4c138584c761df5b511a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/muhammad.ehsan/hr_automation/Lib/site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\muhammad.ehsan\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c197c535b74f4f60afae6b2e51e307d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "379dce7ebed647409e48896d755ef0fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1936de2f2327410fb61de718a6f6d267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a73bdd77313949b8b3a273b37f9f894e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902b62bde8cd444ba47c875637513eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "809f7c93a75946d9a8d2246f01863352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eec0c39a714476cb1f42a026b486d2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f19a035645d4186b9062387049f3dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cfdd43bc304d5db3706cdfa806f16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f9435a58ed74c2e844f6cca43bf627d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The defendant was charged with robbery.\n",
      "The crime took place in 2023.\n",
      "The defense claims the defendant was not present at the crime scene.\n",
      "The prosecution has witnesses that saw the defendant leaving the location of the crime.\n",
      "The court proceedings started in early 2023.\n",
      "Sentences can vary based on various factors, such as the severity of the crime and criminal history.\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Load the pre-trained embedding model\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Function to calculate cosine similarity\n",
    "def cosine_similarity(v1, v2):\n",
    "    return np.dot(v1, v2) / (np.linalg.norm(v1) * np.linalg.norm(v2))\n",
    "\n",
    "# Context-aware chunking using embeddings\n",
    "def context_aware_embedding_chunking(text, similarity_threshold=0.7, max_chunk_size=100):\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_chunk_embedding = np.zeros_like(embeddings[0])\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        current_embedding = embeddings[i]\n",
    "        \n",
    "        if len(current_chunk) == 0:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_embedding = current_embedding\n",
    "            continue\n",
    "        \n",
    "        similarity = cosine_similarity(current_chunk_embedding, current_embedding)\n",
    "        \n",
    "        if similarity > similarity_threshold and len(current_chunk) < max_chunk_size:\n",
    "            current_chunk.append(sentence)\n",
    "            current_chunk_embedding = np.mean(model.encode(current_chunk), axis=0)\n",
    "        else:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = [sentence]\n",
    "            current_chunk_embedding = current_embedding\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"The defendant was charged with robbery. The crime took place in 2023.\n",
    "The defense claims the defendant was not present at the crime scene. The prosecution has witnesses that saw the defendant leaving the location of the crime. \n",
    "The court proceedings started in early 2023. Sentences can vary based on various factors, such as the severity of the crime and criminal history.\"\"\"\n",
    "chunks = context_aware_embedding_chunking(text)\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad8b0846-c904-470d-ae44-5006870f7425",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users/muhammad.ehsan/hr_automation/Lib/site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The defendant was charged with robbery. The defense claims the defendant was not present. Prosecution has witness testimony.\n",
      "The crime took place in 2023.\n",
      "Sentencing can vary based on factors.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def topic_based_embedding_chunking(text, n_topics=3):\n",
    "    sentences = sent_tokenize(text)\n",
    "    embeddings = model.encode(sentences)\n",
    "    \n",
    "    # Clustering sentences based on embeddings\n",
    "    clustering_model = KMeans(n_clusters=n_topics)\n",
    "    clustering_model.fit(embeddings)\n",
    "    \n",
    "    cluster_labels = clustering_model.labels_\n",
    "    \n",
    "    topic_chunks = {}\n",
    "    for i, label in enumerate(cluster_labels):\n",
    "        if label not in topic_chunks:\n",
    "            topic_chunks[label] = []\n",
    "        topic_chunks[label].append(sentences[i])\n",
    "    \n",
    "    return [\" \".join(chunk) for chunk in topic_chunks.values()]\n",
    "\n",
    "# Example text\n",
    "text = \"\"\"The defendant was charged with robbery. The crime took place in 2023.\n",
    "The defense claims the defendant was not present. Sentencing can vary based on factors. Prosecution has witness testimony.\"\"\"\n",
    "chunks = topic_based_embedding_chunking(text)\n",
    "for chunk in chunks:\n",
    "    print(chunk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8efad81b-e7db-4ab0-af82-d8242cd7ecbb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
